{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BIDAF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsEvH-t5jIFq"
      },
      "source": [
        "#Bi-Direction Attention Flow (BiDAF) Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqOKMJEgugm1"
      },
      "source": [
        "#from keras.engine.topology import Layer\n",
        "#from keras.layers.advanced_activations import Softmax\n",
        "#from keras.layers import TimeDistributed, Dense\n",
        "#from keras import backend as K\n",
        "#from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "'''\n",
        "#NNLearner( ARG0, ARG1, BidafOutputLayer(ModellingLayer(BidirectionalAttentionaLayer(  )   )), X, X                 )\n",
        "​\n",
        "#BidirectionalAttentionLayer(layerlist, layerlist2)\n",
        "​\n",
        "# Implementation based off of https://arxiv.org/pdf/1611.01603.pdf\n"
        "#and https://towardsdatascience.com/the-definitive-guide-to-bidaf-part-3-attention-92352bbdcb07\n",
        "'''\n",
        "class BidirectionalAttentionLayer(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    Initializes fairly static class variables\n",
        "    @param activation (neural_network_methods.Activation): the activation function used by the layer\n",
        "    '''\n",
        "    def __init__(self, activation, **kwargs):\n",
        "        super(BidirectionalAttentionLayer, self).__init__(**kwargs)\n",
        "        # get activation function\n",
        "        self.activation = activation\n",
        "\n",
        "    '''\n",
        "    Initializes class variables that are dependent on the input's shape\n",
        "    @param: input_shape (list of lists): [ [None, 2d, T], [None, 2d, J] ]\n",
        "    '''\n",
        "    def build(self, input_shape):\n",
        "        self.d = int(input_shape[0][0] / 2)\n",
        "        self.t = input_shape[0][1]\n",
        "        self.j = input_shape[1][1]\n",
        "        \n",
        "        # creates a trainable weight vector with 6d weights\n",
        "        self.wT = self.add_weight(shape=(1, 6*self.d), initializer=\"random_normal\", trainable=True) # Switched 6d with 1 (Nov 6) Shiyi\n",
        "\n",
        "    '''\n",
        "    Generates the similarity matrix\n",
        "    @param context (2d*T tensor): the embedded context matrix, = H\n",
        "    @param query (2d*J tensor): the embedded query matrix, = U\n",
        "    @return: similarity_matrix (T*J tensor): the matrix of values (how similar two words are to each other), = S\n",
        "    '''\n",
        "    def make_similarity_matrix(self, embedded_context, query):\n",
        "        # similarity is a T*J 2d array of tensors\n",
        "        similarity_matrix = [[0]*self.j]*self.t\n",
        "        \n",
        "        # Fill in every t'th row and j'th column in similarity matrix\n",
        "        # https://www.tensorflow.org/guide/tensor_slicing\n",
        "        # https://stackoverflow.com/questions/41715511/slicing-a-tensor-by-using-indices-in-tensorflow\n",
        "        for cur_t in range(0, self.t):\n",
        "\n",
        "            # context_t is a 1*2d tensor column slice # The dimenstion should be 2d * 1 according to the article\n",
        "\n",
        "            context_column = embedded_context[:, cur_t]\n",
        "            context_column = tf.expand_dims(context_column, axis = -1)\n",
        "            \n",
        "            for cur_j in range(0, self.j):\n",
        "\n",
        "                #query_j is a 1*2d tensor column slice\n",
        "                query_column = query[:, cur_j]\n",
        "\n",
        "                # take the element wise multiplication of the context and query columns\n",
        "                \n",
        "                \n",
        "                \n",
        "                # stack the 1*2d tensor slices onto each other to create a 1*6d tensor\n",
        "                #element_wise_product = tf.expand_dims(element_wise_product, axis = -1) # Promote a vector to 6d * 1 (Nov 6)\n",
        "                query_column = tf.expand_dims(query_column, axis = -1) # Promote a vector to 6d * 1 (Nov 6)\n",
        "                # take the element wise multiplication of the context and query columns\n",
        "                element_wise_product = context_column * query_column\n",
        "                concatenated_vector = tf.concat([context_column, query_column, element_wise_product], 0) # Changed from 1 to 0 (Nov 6)\n",
        "                \n",
        "                #matrix_concatenated_vector = tf.expand_dims(concatenated_vector, axis = -1) # Promote a vector to 6d * 1 (Nov 6)\n",
        "                \n",
        "                # cross-multiply the trainable weight vector with the 1*6d tensor, resulting in a scalar\n",
        "                entry = tf.matmul(self.wT, concatenated_vector) # switched order (Nov 6) \n",
        "                \n",
        "\n",
        "                # https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update\n",
        "                similarity_matrix[cur_t][cur_j] = entry[0][0]\n",
        "\n",
        "        # return a T*J tensor\n",
        "        return tf.squeeze(tf.convert_to_tensor(similarity_matrix))\n",
        "\n",
        "    '''\n",
        "    Generates the context to query attention matrix\n",
        "    @param query (2d*J tensor): the embedded query matrix, = U\n",
        "    @param similarity_matrix (T*J tensor): the matrix of values (how similar two words are to each other), = S\n",
        "    @return c2q (2d*T tensor): the context to query attention matrix, = U_hat\n",
        "    '''\n",
        "    def contextToQueryAttention(self, embedded_query, similarity_matrix):\n",
        "        # T*2d matrix of tensors\n",
        "        c2q = [[0]*2*self.d]*self.t\n",
        "\n",
        "        # take the softmax of every row of the similarity matrix\n",
        "        for i in range(self.t):\n",
        "            # softmax(ith row) of similarity matrix\n",
        "            \n",
        "            softmaxed_row = tf.nn.softmax(similarity_matrix[i]) # changed softmax lib (Nov 6)\n",
        "            \n",
        "            # take the weighted sum of the weights times the columns of query\n",
        "            # yes, tensors can be multiplied by scalars and added like this, it's kinda cool\n",
        "            weighted_sum = sum([softmaxed_row[j] * embedded_query[:,j] for j in range(0, self.j)])\n",
        "\n",
        "            # put the values into c2q\n",
        "            c2q[i] = weighted_sum\n",
        "\n",
        "        # convert c2q into a T*2d tensor\n",
        "        c2q = tf.squeeze(tf.convert_to_tensor(c2q))\n",
        "\n",
        "        # return a 2d*T tensor\n",
        "        return tf.transpose(c2q)\n",
        "\n",
        "    '''\n",
        "    Generates the query to context attention matrix\n",
        "    @param context (2d*T tensor): the embedded query matrix, = H\n",
        "    @param similarity_matrix (T*J tensor): the matrix of values (how similar two words are to each other), = S\n",
        "    @return q2c (2d*T tensor): the query to context attention matrix, = H_hat\n",
        "    '''\n",
        "    def queryToContextAttention(self, embedded_context, similarity_matrix):\n",
        "\n",
        "        # Take maximum of each row and output single vector containing these maximums\n",
        "        # 1*T vector representing max across each row, = z in the paper\n",
        "        attention_values = [max(similarity_matrix[i]) for i in range(0,self.t)]\n",
        "\n",
        "        # 1*T vector, the softmax the row of maximums, = b\n",
        "        attention_distribution = tf.nn.softmax(attention_values)\n",
        "\n",
        "        # 1*2d weighted sum vector\n",
        "        weighted_sum = sum([attention_distribution[i] * embedded_context[:,i] for i in range(0, self.t)])\n",
        "\n",
        "        # duplicate T times\n",
        "        q2c = [weighted_sum] * self.t\n",
        "      \n",
        "        # convert to T*2d tensor\n",
        "        q2c = tf.squeeze(tf.convert_to_tensor(q2c))\n",
        "\n",
        "        # return 2d*T tensor\n",
        "        return tf.transpose(q2c)\n",
        "\n",
        "    '''\n",
        "    Generates the query to context attention matrix\n",
        "    @param context (2d*T tensor): the embedded query matrix, = H\n",
        "    @param c2q (2d*T tensor): the context to query attention matrix, = U_hat\n",
        "    @param q2c (2d*T tensor): the query to context attention matrix, = H_hat\n",
        "    @return megamerged_matrix (8d*T tensor): the megamerged matrix, = G\n",
        "    '''\n",
        "    def megamergeStep(self, embedded_context, c2q, q2c):\n",
        "        \"\"\"\n",
        "        Merge Context, Query --> Context, Context --> Query matrices into 8dxT matrix\n",
        "​\n",
        "        H_hat = query --> context attention\n",
        "        U_hat = context --> query attention\n",
        "        context matrix\n",
        "​\n",
        "        How to get 8dxT matrix\n",
        "​\n",
        "        Grab the t-th column of H\n",
        "        Grab the t-th column of U_hat\n",
        "        Do the elementwise product of t-th col of U_hat and t-th col of H\n",
        "        Do the elementwise product of t-th col of H_hat and t-th column of H\n",
        "        Grab the t-th column of H_hat matrix\n",
        "​\n",
        "        The order I've mentioned is how to stack the columns. Repeat for all columns up in range(t)\n",
        "        \"\"\"\n",
        "        \n",
        "        # T*2d megamerged matrix G\n",
        "        megamerged_matrix = [[0]*8*self.d]*self.t\n",
        "\n",
        "        # merge columns of embedded_context, c2q, and q2c\n",
        "        for i in range(self.t):\n",
        "            megamerged_matrix[i] = tf.concat([embedded_context[:,i], c2q[:,i], c2q[:,i]*embedded_context[:,i], q2c[:,i]*embedded_context[:,i]], 0) # changed to zero (Nov 6)\n",
        "\n",
        "        # convert into T*8d tensor\n",
        "        megamerged_matrix = tf.squeeze(tf.convert_to_tensor(megamerged_matrix))\n",
        "\n",
        "        # return 8d*T tensor\n",
        "        return tf.transpose(megamerged_matrix)\n",
        "\n",
        "    def call(self, input):\n",
        "        embedded_query = input[1]     # U\n",
        "        embedded_context = input[0]   # H\n",
        "        \n",
        "        similarity_matrix = self.make_similarity_matrix(embedded_context, embedded_query)\n",
        "        # Context --> Query Attention. U_hat = Attended query matrix for all context words\n",
        "        c2q = self.contextToQueryAttention(embedded_query, similarity_matrix)\n",
        "\n",
        "        # Query --> Context Attention\n",
        "        q2c = self.queryToContextAttention(embedded_context, similarity_matrix)\n",
        "\n",
        "        megamerged = self.megamergeStep(embedded_context, c2q, q2c)\n",
        "\n",
        "        return megamerged\n",
        "\n",
        "\n",
        "class BidafOutput(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(BidafOutput, self).__init__(kwargs)\n",
        "\n",
        "    def build(self, inputs_shape):\n",
        "        \n",
        "        self.d = int(inputs_shape[0][0]/8)\n",
        "        self.t = inputs_shape[1][1]\n",
        "\n",
        "        self.w_p1 = self.add_weight(shape=(1, 10*self.d), initializer=\"random_normal\", trainable=True)\n",
        "        self.w_p2 = self.add_weight(shape=(1, 10*self.d), initializer=\"random_normal\", trainable=True)\n",
        "\n",
        "        super(BidafOutput, self).build(inputs_shape)\n",
        "\n",
        "    def get_answer(self, p1, p2, context):\n",
        "        p1_max_index = np.argmax(p1)\n",
        "        p2_max_index = np.argmax(p2)\n",
        "\n",
        "        if p1_max_index > p2_max_index:\n",
        "            p1_max_index = 0\n",
        "            p2_max_index = 0\n",
        "            max_prob = 0\n",
        "            for i in range(0, self.t):\n",
        "                for j in range(i, self.t):\n",
        "                    if p1[i]*p2[j] > max_prob:\n",
        "                        p1_max_index = i\n",
        "                        p2_max_index = j\n",
        "\n",
        "        return context[p1_max_index:p2_max_index+1]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # https://stackoverflow.com/questions/44747343/keras-input-explanation-input-shape-units-batch-size-dim-etc\n",
        "        # important discussion of dimensions\n",
        "        # https://towardsdatascience.com/modeling-and-output-layers-in-bidaf-an-illustrated-guide-with-minions-f2e101a10d83\n",
        "        # https://arxiv.org/pdf/1611.01603.pdf is slightly different: has M2 calculated in output layer rather than in modeling layer\n",
        "        # in this case order (where you calculate M2) doesn't matter\n",
        "        \n",
        "        G, M1, M2, context = inputs\n",
        "\n",
        "\n",
        "        # concatenate G and M1 to form a 10*T tensor\n",
        "        G_M1 = tf.concat([G, M1], axis=0)\n",
        "        G_M2 = tf.concat([G, M2], axis=0)\n",
        "        \n",
        "        # initialize two length T arrays of tensors\n",
        "        p1 = [0.0] * self.t\n",
        "        p2 = [0.0] * self.t\n",
        "\n",
        "        \n",
        "        # set values of p1 and p2\n",
        "        for i in range(0, self.t):\n",
        "            promoted_G_M1_col = tf.expand_dims(G_M1[:,i], axis = -1)\n",
        "            promoted_G_M2_col = tf.expand_dims(G_M2[:,i], axis = -1)\n",
        "            p1[i] = tf.matmul(self.w_p1, promoted_G_M1_col)\n",
        "            p1[i] = tf.matmul(self.w_p2, promoted_G_M2_col)\n",
        "\n",
        "        \n",
        "        # convert to length T tensor\n",
        "        p1 = tf.squeeze(tf.convert_to_tensor(p1))\n",
        "        p2 = tf.squeeze(tf.convert_to_tensor(p2))\n",
        "\n",
        "        p1 = tf.expand_dims(p1, axis = -1)\n",
        "        p2 = tf.expand_dims(p2, axis = -1)\n",
        "        p1 = tf.reshape(p1, [p1.shape[1], p1.shape[0]])\n",
        "        p2 = tf.reshape(p2, [p2.shape[1], p2.shape[0]])\n",
        "\n",
        "\n",
        "        # take softmax of p1\n",
        "        p1 = tf.nn.softmax(p1)\n",
        "        p2 = tf.nn.softmax(p2)\n",
        "\n",
        "        # https://stackoverflow.com/questions/34097281/convert-a-tensor-to-numpy-array-in-tensorflow\n",
        "        # at the end convert the probabilities into numpy arrays to do calculations on\n",
        "        # Now we want to return p1 and p2 instead of the context details (from get answer)\n",
        "\n",
        "        return p1, p2\n",
        "        # return self.get_answer(p1.numpy(), p2.numpy(), context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPSQJXi8CQKU"
      },
      "source": [
        "from keras.layers import Layer\n",
        "from keras.layers import Dropout, LSTM, Bidirectional\n",
        "from tensorflow.keras.regularizers import l2 \n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "'''\n",
        "def BidafModelingFunc(output_dim, layerlist, activation = 'tanh', regularizer = 0, dropout=0.2):\n",
        "    \"\"\"\n",
        "    Create Bidaf Modeling layer\n",
        "    \"\"\"\n",
        "    layerlist.append(BidafModeling(output_dim, activation=activation, regularizer=regularizer, dropout=dropout))\n",
        "    return layerlist\n",
        "'''\n",
        "\n",
        "class BidafModeling(Layer):\n",
        "    def __init__(self, out_dim, activation, dropout = 0.2, regularizer = 0, **kwargs):\n",
        "        super(BidafModeling, self).__init__(kwargs)\n",
        "        if regularizer == 0:\n",
        "            regularizer = None\n",
        "        else:\n",
        "            regularizer = l2(10**(-1*regularizer))\n",
        "        self.lstm_1 = Bidirectional(LSTM(units = out_dim, activation=activation, kernel_regularizer=regularizer, dropout=dropout))\n",
        "        self.lstm_2 = Bidirectional(LSTM(units = out_dim, activation=activation, kernel_regularizer=regularizer))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = tf.reshape(inputs, [inputs.shape[1], inputs.shape[0], 1])\n",
        "        lstm_1_output = self.lstm_1(inputs)\n",
        "        lstm_1_output = tf.expand_dims(lstm_1_output, -1) #M1\n",
        "        lstm_2_output = self.lstm_2(lstm_1_output) #M2\n",
        "        #now return a tuple of matrices M1 and M2\n",
        "        return tf.reshape(lstm_1_output, [lstm_1_output.shape[1], lstm_1_output.shape[0]]), tf.reshape(lstm_2_output, [lstm_2_output.shape[1], lstm_2_output.shape[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2xpT_T5kpyh"
      },
      "source": [
        "#Standalone Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IG8vR-RoLKiO",
        "outputId": "74b27ee5-ca4e-420e-9088-e7fa2e582e19"
      },
      "source": [
        "dummy_query_embedding = np.ones((2, 5))\n",
        "squeeze_query = tf.squeeze(tf.convert_to_tensor(dummy_query_embedding))\n",
        "squeeze_query"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 5), dtype=float64, numpy=\n",
              "array([[1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1.]])>"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJBK5TmHLUQI",
        "outputId": "81480597-cbe4-4dfa-963f-b64c3960c5c6"
      },
      "source": [
        "dummy_context_embedding = np.ones((2, 7))\n",
        "squeeze_context = tf.squeeze(tf.convert_to_tensor(dummy_context_embedding))\n",
        "squeeze_context"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 7), dtype=float64, numpy=\n",
              "array([[1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1.]])>"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQZPI3_OGtMW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1c29401-b3d0-4dae-da60-c2c2ede64ae1"
      },
      "source": [
        "# Bi-directional\n",
        "test = BidirectionalAttentionLayer(activation = \"tanh\")\n",
        "bidirectionalOut = test([squeeze_context, squeeze_query])\n",
        "print(\"Dimensions of the bidirectional attention layer output: \" + str(bidirectionalOut.shape) + \" with expected dimension: (8, 7)\")\n",
        "\n",
        "# Modelling\n",
        "test = BidafModeling(out_dim = 1, activation = \"tanh\")\n",
        "M1, M2 = test(bidirectionalOut)\n",
        "print(\"Dimensions of the modeling layer output: \" + str(M1.shape) + str(M2.shape) + \" with expected dimension: (2, 7)(2, 7)\")\n",
        "\n",
        "# Output\n",
        "outputIn = [bidirectionalOut, M1, M2, squeeze_context]\n",
        "test = BidafOutput()\n",
        "P1, P2 = test(outputIn)\n",
        "\n",
        "print(\"Dimensions of the output layer output: \" + str(P1.shape) + str(P2.shape) + \" with expected dimension: (1, 7)(1, 7)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions of the bidirectional attention layer output: (8, 7) with expected dimension: (8, 7)\n",
            "Dimensions of the modeling layer output: (2, 7)(2, 7) with expected dimension: (2, 7)(2, 7)\n",
            "Dimensions of the output layer output: (1, 7)(1, 7) with expected dimension: (1, 7)(1, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhamSGaodwNj"
      },
      "source": [
        "#Unit Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PXzX7Q9eQ5w"
      },
      "source": [
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdWJF5_OFYR8"
      },
      "source": [
        "def test_pipeline_dim():\n",
        "  d = random.randint(2, 100)\n",
        "  t = random.randint(2, 100)\n",
        "  j = random.randint(2, 100)\n",
        "  context_embed = np.ones((2*d, t))\n",
        "  query_embed = np.ones((2*d, j))\n",
        "  context_tensor = tf.squeeze(tf.convert_to_tensor(context_embed))\n",
        "  query_tensor = tf.squeeze(tf.convert_to_tensor(query_embed))\n",
        "\n",
        "  #Bidirectional Attention\n",
        "  test = BidirectionalAttentionLayer(activation = \"tanh\")\n",
        "  bidirectionalOut = test([context_tensor, query_tensor])\n",
        "  assert bidirectionalOut.shape[0] == 8*d #8dxT\n",
        "  assert bidirectionalOut.shape[1] == t \n",
        "\n",
        "  #Modeling Layer\n",
        "  test = BidafModeling(out_dim = d, activation = \"tanh\")\n",
        "  M1, M2 = test(bidirectionalOut)\n",
        "  assert M1.shape[0] == 2*d #2dxT\n",
        "  assert M2.shape[0] == 2*d \n",
        "  assert M1.shape[1] == t\n",
        "  assert M2.shape[1] == t \n",
        "\n",
        "  #Output Layer\n",
        "  outputIn = [bidirectionalOut, M1, M2, context_tensor]\n",
        "  test = BidafOutput()\n",
        "  P1, P2 = test(outputIn)\n",
        "  assert P1.shape[0] == 1 #1xt\n",
        "  assert P1.shape[1] == t \n",
        "  assert P2.shape[0] == 1\n",
        "  assert P2.shape[1] == t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcu8TpR_fmRQ",
        "outputId": "b320f1d9-e27b-4e4d-8f76-1c959d398a86"
      },
      "source": [
        "num_test = 10\n",
        "print(\"Running \" + str(num_test) + \" random input PyTests...\")\n",
        "\n",
        "for i in range(num_test):\n",
        "  print(\"Starting test \" + str(i + 1))\n",
        "  test_pipeline_dim()\n",
        "  print(\"-> Test \" + str(i + 1) + \" passed!\")\n",
        "\n",
        "print(\"Success! All tests passed!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running 10 random input PyTests...\n",
            "Starting test 1\n",
            "-> Test 1 passed!\n",
            "Starting test 2\n",
            "-> Test 2 passed!\n",
            "Starting test 3\n",
            "-> Test 3 passed!\n",
            "Starting test 4\n",
            "-> Test 4 passed!\n",
            "Starting test 5\n",
            "-> Test 5 passed!\n",
            "Starting test 6\n",
            "-> Test 6 passed!\n",
            "Starting test 7\n",
            "-> Test 7 passed!\n",
            "Starting test 8\n",
            "-> Test 8 passed!\n",
            "Starting test 9\n",
            "-> Test 9 passed!\n",
            "Starting test 10\n",
            "-> Test 10 passed!\n",
            "Success! All tests passed!\n"
          ]
        }
      ]
    }
  ]
}
